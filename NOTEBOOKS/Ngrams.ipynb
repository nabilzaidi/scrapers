{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import spacy\n",
    "import wordcloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://gist.github.com/benhoyt/dfafeab26d7c02a52ed17b6229f0cb52\"\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"Convert string to lowercase and split into words (ignoring\n",
    "    punctuation), returning list of words.\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\w+', string.lower())\n",
    "\n",
    "\n",
    "def count_ngrams(lines, min_length=2, max_length=4):\n",
    "    \"\"\"Iterate through given lines iterator (file object or list of\n",
    "    lines) and return n-gram frequencies. The return value is a dict\n",
    "    mapping the length of the n-gram to a collections.Counter\n",
    "    object of n-gram tuple and number of times that n-gram occurred.\n",
    "    Returned dict includes n-grams of length min_length to max_length.\n",
    "    \"\"\"\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "    ngrams = {length: collections.Counter() for length in lengths}\n",
    "    queue = collections.deque(maxlen=max_length)\n",
    "\n",
    "    # Helper function to add n-grams at start of current queue to dict\n",
    "    def add_queue():\n",
    "        current = tuple(queue)\n",
    "        for length in lengths:\n",
    "            if len(current) >= length:\n",
    "                ngrams[length][current[:length]] += 1\n",
    "\n",
    "    # Loop through all lines and words and add n-grams to dict\n",
    "    for line in lines:\n",
    "        for word in tokenize(line):\n",
    "            queue.append(word)\n",
    "            if len(queue) >= max_length:\n",
    "                add_queue()\n",
    "\n",
    "    # Make sure we get the n-grams at the tail end of the queue\n",
    "    while len(queue) > min_length:\n",
    "        queue.popleft()\n",
    "        add_queue()\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def print_most_frequent(ngrams, num=50):\n",
    "    \"\"\"Print num most common n-grams of each length in n-grams dict.\"\"\"\n",
    "    for n in sorted(ngrams):\n",
    "        print('----- {} most common {}-grams -----'.format(num, n))\n",
    "        for gram, count in ngrams[n].most_common(num):\n",
    "            print('{0}: {1}'.format(' '.join(gram), count))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"trustpilot_tripmate.xlsx\"\n",
    "df = pd.read_excel(filename, index_col=0, engine=\"openpyxl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "stop_words = [unidecode(w.lower()) for w in stop_words]\n",
    "def remove_stopwords(rev):\n",
    "    rev_new = \" \".join([i for i in rev.lower().split() if i not in stop_words and len(i)>3])\n",
    "    return rev_new\n",
    "\n",
    "df[\"reviews_cleaned\"] = df['text_cleaned_hide_EN'].copy()\n",
    "\n",
    "# clean reviews\n",
    "df[\"reviews_cleaned\"] = df['reviews_cleaned'].apply(lambda x: \" \" + ' '.join([unidecode(w.lower()) for w in x.split()]))\n",
    "\n",
    "words_toremove = [\"lauritz\", \"lauritzcom\", \"dba\", \"tradera\", \"block\", \n",
    "                  \"auction\", \"good\"]\n",
    "\n",
    "for word in words_toremove:\n",
    "    df[\"reviews_cleaned\"] = df[\"reviews_cleaned\"].apply(lambda x: re.sub(f\"{word}\", \" \", x))\n",
    "\n",
    "# remove stopwords from the text\n",
    "df[\"reviews_cleaned\"] = df[\"reviews_cleaned\"].apply(remove_stopwords)\n",
    "\n",
    "df[\"reviews_cleaned\"] = df[\"reviews_cleaned\"].apply(lambda x: re.sub(r\"[^a-z\\s]\", \"\", x))\n",
    "\n",
    "df[\"reviews_cleaned\"] = df[\"reviews_cleaned\"].apply(lambda x: re.sub(\"\\s+\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(df[df.rating_star_cleaned_hide < 4][\"reviews_cleaned\"]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = count_ngrams(text)\n",
    "print_most_frequent(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
